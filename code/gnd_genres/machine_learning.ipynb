{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conduct machine learning.__\n",
    "\n",
    "@Andreas LÃ¼schow\n",
    "\n",
    "12.11.2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV = \"./data/all_data.csv\"\n",
    "GENRE_FILE = \"./data/input/genres.txt\"\n",
    "FIELD_FILE = \"./data/input/fields.txt\"\n",
    "\n",
    "TRAIN_FIELD_LABEL = 'genre'  # genre | genre_main\n",
    "\n",
    "MAX_GENRES = 25\n",
    "ML = \"d_tree\"  # logreg | random_forest | d_tree\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# reduce data size if necessary\n",
    "SAMPLING = False\n",
    "SAMPLING_FRACTION = 1  # only relevant if SAMPLING == True\n",
    "\n",
    "DROP_NAN = True\n",
    "DROP_NAN_THRESHOLD = 1000  # only relevant if DROP_NAN == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [line.strip() for line in open(GENRE_FILE)]\n",
    "assert len(genres) == 1319\n",
    "print(genres[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_df = pd.read_csv(INPUT_CSV, sep=\"\\t\", index_col='Unnamed: 0', low_memory=False)\n",
    "src_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = src_df.copy()\n",
    "orig_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace in genre names\n",
    "orig_df[\"genre\"] = orig_df[\"genre\"].str.replace(\" \", \"_\")\n",
    "orig_df[\"genre_main\"] = orig_df[\"genre_main\"].str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert src_df.shape[0] == 746786\n",
    "assert orig_df.shape[0] == 746786"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLING:\n",
    "    orig_df = orig_df.sample(frac=SAMPLING_FRACTION, replace=True, random_state=1)\n",
    "    orig_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns/fields where NaN is predominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DROP_NAN:\n",
    "    orig_df.dropna(thresh=len(orig_df.index)/DROP_NAN_THRESHOLD, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by genre count\n",
    "genres_top = orig_df[TRAIN_FIELD_LABEL].value_counts()[:MAX_GENRES].index.tolist()\n",
    "df_tmp = orig_df[orig_df[TRAIN_FIELD_LABEL].isin(genres_top)]\n",
    "\n",
    "# order\n",
    "i, r = pd.factorize(df_tmp[TRAIN_FIELD_LABEL])\n",
    "a = np.argsort(np.bincount(i)[i], kind='mergesort')\n",
    "df_tmp = df_tmp.iloc[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final data\n",
    "y = df_tmp.loc[:,TRAIN_FIELD_LABEL].values\n",
    "df = df_tmp.notnull().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ML == \"logreg\":\n",
    "    model = LogisticRegression()\n",
    "elif ML == \"random_forest\":\n",
    "    model = RandomForestClassifier()\n",
    "elif ML == \"d_tree\":\n",
    "    model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy Score: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_report = classification_report(y_test, predictions)\n",
    "# print(c_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "# cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision_score(y_test, predictions, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, predictions, average='weighted')}\")\n",
    "print(f\"F1-score: {f1_score(y_test, predictions, average='weighted')}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_report(cr, title=\"\", with_avg_total=False, cmap=plt.cm.Blues):\n",
    "    \"\"\"see https://stackoverflow.com/a/31689645\"\"\"\n",
    "    lines = cr.split('\\n')\n",
    "\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    for line in lines[2 : (len(lines) - 5)]:\n",
    "        # print(line)\n",
    "        t = line.split()\n",
    "        # print(t)\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        print(v)\n",
    "        plotMat.append(v)\n",
    "\n",
    "    if with_avg_total:\n",
    "        aveTotal = lines[len(lines) - 1].split()\n",
    "        classes.append('avg/total')\n",
    "        vAveTotal = [float(x) for x in t[1:len(aveTotal) - 1]]\n",
    "        plotMat.append(vAveTotal)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.imshow(plotMat, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    x_tick_marks = np.arange(3)\n",
    "    y_tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(x_tick_marks, ['Precision', 'Recall', 'F1-score'], rotation=90)\n",
    "    plt.yticks(y_tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Genres')\n",
    "\n",
    "plot_classification_report(c_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm)\n",
    "# consider relative values in confusion matrix\n",
    "cm_relative = cm_df.loc[:].div(cm_df.sum(axis='columns'), axis=\"index\")\n",
    "# cm_relative[:2]\n",
    "cm_relative.to_csv('./data/confusion_matrix_accuracy.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,15))\n",
    "ax = sns.heatmap(cm_relative, annot=True, xticklabels=genres_top, yticklabels=genres_top, fmt=\".2f\", cmap=\"Greens\")\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom+0.5, top-0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
